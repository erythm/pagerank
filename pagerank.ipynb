{"cells":[{"cell_type":"markdown","id":"b727e29a","metadata":{"id":"b727e29a"},"source":["# PageRank Algorithm Implementation and Exercises\n","\n","In this notebook we study and implement the PageRank algorithm following the formulation described by Bryan and Leise in **“The $25{,}000{,}000{,}000$ Eigenvector: The Linear Algebra Behind Google”**.  \n","The objective is to provide a mathematically sound implementation of the algorithm, validate it on the example graphs presented in the paper, and solve selected exercises.\n","\n","We construct small link matrices corresponding to Figures 2.1 and 2.2, implement the PageRank computation using the power method, and solve Exercises 11 and 14 from the paper. An additional cell allows the algorithm to be tested on a real dataset (e.g. the Hollins University web graph), provided that a file named `hollins.dat` is available in the working directory.\n","\n","The PageRank algorithm computes the stationary distribution of a stochastic matrix\n","$$\n","M = (1 - m)\\,A + m\\,S\n","$$\n","where $(A) $ is a column-stochastic link matrix representing the web graph, $(S) $ is the teleportation matrix with all entries equal to \\(1/n\\), and $(m \\in (0,1)) $$ is the damping factor (typically $ (m = 0.15)$).  \n","The PageRank vector is defined as the dominant eigenvector of $(M) $ associated with the eigenvalue 1, and it is computed numerically using the power method.\n"]},{"cell_type":"markdown","id":"2610d036","metadata":{"id":"2610d036"},"source":["## Construction of the Link Matrices and PageRank Implementation\n","\n","In this section we define the link matrices corresponding to the example graphs presented in the paper and implement the PageRank algorithm using the power method.\n","\n","### Link Matrices for the Example Graphs\n","\n","For the small examples in Figures 2.1 and 2.2, the web is represented by a directed graph and encoded in a matrix $$(A \\in \\mathbb{R}^{n \\times n})$$ .  \n","The convention used throughout this notebook is the following:\n","\n","- Each column $(j)$ represents page $(j)$ .\n","- The nonzero entries in column $(j)$ indicate the pages reached by outgoing links from page $(j)$.\n","- If page $(j)$ has $(d_j)$ outgoing links, each destination receives a weight $(1/d_j)$.\n","\n","Using this convention, the matrices for Figures 2.1 and 2.2 are constructed explicitly by assigning the appropriate nonzero entries.  \n","Exercise 11 is handled by extending the matrix of Figure 2.1 with an additional page and modifying the corresponding links as described in the exercise.\n","\n","### Handling of Dangling Nodes\n","\n","A page with no outgoing links corresponds to a zero column in the matrix $(A)$.  \n","Such pages are called *dangling nodes* and would break the stochasticity of the matrix. To handle this case, any zero column is replaced by a uniform distribution over all pages. This ensures that the resulting matrix is column-stochastic.\n","\n","### PageRank Computation via the Power Method\n","\n","Given a column-stochastic matrix $(A)$, the PageRank algorithm is based on the Google matrix\n","$$\n","M = (1 - m)A + mS\n","$$\n","where $(m)$ is the damping factor and $(S)$ is the matrix whose entries are all equal to $(1/n)$.\n","\n","The PageRank vector is defined as the dominant eigenvector of $(M)$ associated with the eigenvalue 1.  \n","It is computed numerically using the power method, starting from a uniform probability vector. At each iteration, the vector is normalized to avoid numerical drift, and convergence is checked using the $( \\ell^1 )$-norm of the difference between successive iterates.\n"]},{"cell_type":"code","execution_count":1,"id":"332b3b45","metadata":{"id":"332b3b45","executionInfo":{"status":"ok","timestamp":1767179357470,"user_tz":-60,"elapsed":87,"user":{"displayName":"Erf","userId":"14792764007912314216"}}},"outputs":[],"source":["import numpy as np\n","from typing import Tuple\n","\n","def build_matrix_fig21() -> np.ndarray:\n","    \"\"\"Return the 4×4 link matrix for Figure 2.1: each column j lists the destinations of page j.\"\"\"\n","    A = np.zeros((4, 4), dtype=float)\n","    # Page 1 links to 2,3,4 with equal weight\n","    A[1, 0] = A[2, 0] = A[3, 0] = 1/3\n","    # Page 2 links to 3,4\n","    A[2, 1] = A[3, 1] = 1/2\n","    # Page 3 links only to 1\n","    A[0, 2] = 1.0\n","    # Page 4 links to 1 and 3\n","    A[0, 3] = A[2, 3] = 1/2\n","    return A\n","\n","def build_matrix_fig22() -> np.ndarray:\n","    \"\"\"Return the 5×5 link matrix for Figure 2.2 (two disconnected subwebs).\"\"\"\n","    A = np.zeros((5, 5), dtype=float)\n","    # Subweb 1: pages 1↔2\n","    A[1, 0] = 1.0\n","    A[0, 1] = 1.0\n","    # Subweb 2: pages 3↔4\n","    A[3, 2] = 1.0\n","    A[2, 3] = 1.0\n","    # Page 5 links to 3 and 4 with equal weight\n","    A[2, 4] = A[3, 4] = 0.5\n","    return A\n","\n","def build_matrix_exercise11() -> np.ndarray:\n","    \"\"\"Return the 5×5 link matrix for Exercise 11 (Figure 2.1 + a fifth page).\"\"\"\n","    A = np.zeros((5, 5), dtype=float)\n","    # Original four pages\n","    A[1, 0] = A[2, 0] = A[3, 0] = 1/3\n","    A[2, 1] = A[3, 1] = 1/2\n","    # Page 3 now links to pages 1 and 5\n","    A[0, 2] = A[4, 2] = 1/2\n","    # Page 4 links to pages 1 and 3\n","    A[0, 3] = A[2, 3] = 1/2\n","    # New page 5 links only to page 3\n","    A[2, 4] = 1.0\n","    return A\n","\n","def ensure_column_stochastic(A: np.ndarray) -> np.ndarray:\n","    \"\"\"Return a column‑stochastic version of A: normalizes nonzero columns, replaces zero-columns with uniform.\"\"\"\n","    n = A.shape[0]\n","    B = np.zeros_like(A, dtype=float)\n","    for j in range(n):\n","        col = A[:, j]\n","        s = col.sum()\n","        if s > 0:\n","            B[:, j] = col / s\n","        else:\n","            B[:, j] = 1.0 / n  # dangling node: distribute uniformly\n","    return B\n","\n","def pagerank(A: np.ndarray, m: float = 0.15, tol: float = 1e-12, max_iter: int = 10000) -> Tuple[np.ndarray, int]:\n","    \"\"\"\n","    Compute the PageRank vector for the column‑stochastic matrix A using the power method.\n","\n","    Parameters\n","    ----------\n","    A : np.ndarray\n","        Column‑stochastic link matrix.\n","    m : float\n","        Damping factor (default 0.15).\n","    tol : float\n","        Convergence tolerance in L1 norm (default 1e-12).\n","    max_iter : int\n","        Maximum number of power iterations (default 10,000).\n","\n","    Returns\n","    -------\n","    v : np.ndarray\n","        PageRank vector.\n","    k : int\n","        Number of iterations used.\n","    \"\"\"\n","    # Ensure A is column‑stochastic for safety\n","    A = ensure_column_stochastic(A)\n","    n = A.shape[0]\n","    # Google matrix\n","    S = np.ones((n, n), dtype=float) / n\n","    M = (1 - m) * A + m * S\n","    # Start from uniform distribution\n","    v = np.ones(n, dtype=float) / n\n","    for k in range(1, max_iter + 1):\n","        v_next = M @ v\n","        v_next = v_next / v_next.sum()  # normalize to avoid drift\n","        if np.abs(v_next - v).sum() < tol:\n","            return v_next, k\n","        v = v_next\n","    return v, max_iter\n"]},{"cell_type":"markdown","id":"0d10840d","metadata":{"id":"0d10840d"},"source":["## PageRank Computation on a Real Dataset\n","\n","To apply the PageRank algorithm to a real web graph, the dataset is provided as an edge list and converted into an adjacency-list representation. Each line of the file represents a directed link between two nodes.\n","\n","### Loading the Dataset\n","\n","The dataset is parsed and stored as a dictionary mapping each node to the list of its outgoing neighbors.  \n","During this process:\n","- Commented or malformed lines are ignored.\n","- Self-loops can be optionally removed.\n","- The direction of edges can be flipped if needed, depending on the dataset convention.\n","\n","This representation is suitable for efficient iteration over outgoing links and scales well to large graphs.\n","\n","### PageRank Power Method on Adjacency Lists\n","\n","For large datasets, explicitly storing the matrix $(A)$ is inefficient. Instead, the PageRank iteration is implemented directly on the adjacency list.\n","\n","Given the current PageRank vector $(v) $, the update step is:\n","$$\n","v^{(k+1)} = (1 - m)\\,\\hat{A}v^{(k)} + \\frac{m}{n}\\mathbf{1}\n","$$\n","where \\(\\hat{A}\\) denotes the implicit column-stochastic link structure.\n","\n","Pages with no outgoing links (dangling nodes) are handled by accumulating their PageRank mass and redistributing it uniformly across all nodes at each iteration. This preserves stochasticity and guarantees convergence.\n","\n","At each iteration:\n","- The vector is normalized to avoid numerical drift.\n","- Convergence is checked using the \\( \\ell^1 \\)-norm:\n","$$\n","\\|v^{(k+1)} - v^{(k)}\\|_1 < \\text{tol}\n","$$\n","\n","The algorithm stops once the tolerance is met or the maximum number of iterations is reached.\n","\n","### Dataset Availability\n","\n","The dataset is loaded only if a file named `hollins.dat` is found in the working directory. If the file is not present, the dataset analysis is skipped without affecting the rest of the notebook.\n"]},{"cell_type":"code","execution_count":2,"id":"609f9fc4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"609f9fc4","executionInfo":{"status":"ok","timestamp":1767179363450,"user_tz":-60,"elapsed":73,"user":{"displayName":"Erf","userId":"14792764007912314216"}},"outputId":"d66de955-0fc6-49c5-fb9b-e38ecbc797fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["file_path = hollins.dat\n"]}],"source":["import os\n","import numpy as np\n","from typing import Dict, List, Tuple\n","\n","def load_edge_list_as_adj(path: str, *, flip_direction: bool = False, drop_self_loops: bool = True) -> Dict[int, List[int]]:\n","    edges: List[Tuple[int, int]] = []\n","    nodes: set[int] = set()\n","    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n","        for raw in f:\n","            line = raw.strip()\n","            if not line or line.startswith('#') or line.startswith('//') or line.startswith('%'):\n","                continue\n","            parts = line.split()\n","            if len(parts) < 2:\n","                continue\n","            try:\n","                a = int(parts[0]); b = int(parts[1])\n","            except ValueError:\n","                continue\n","            u, v = (b, a) if flip_direction else (a, b)\n","            if drop_self_loops and u == v:\n","                continue\n","            edges.append((u, v))\n","            nodes.update({u, v})\n","    adj: Dict[int, List[int]] = {node: [] for node in sorted(nodes)}\n","    for u, v in edges:\n","        adj[u].append(v)\n","    return adj\n","\n","def pagerank_power(adj: Dict[int, List[int]], m: float = 0.15, tol: float = 1e-12, max_iter: int = 1000) -> Tuple[Dict[int, float], int, float]:\n","    nodes = list(adj.keys())\n","    n = len(nodes)\n","    idx = {node: i for i, node in enumerate(nodes)}\n","    outdeg = [len(adj[node]) for node in nodes]\n","\n","    v = np.ones(n) / n\n","    teleport = m / n\n","    one_minus_m = 1.0 - m\n","\n","    last_diff = float(\"inf\")\n","    it = 0\n","\n","    for it in range(1, max_iter + 1):\n","        v_next = np.zeros(n)\n","        dangling_mass = 0.0\n","\n","        for j, node in enumerate(nodes):\n","            if outdeg[j] == 0:\n","                dangling_mass += v[j]\n","            else:\n","                share = v[j] / outdeg[j]\n","                for dest in adj[node]:\n","                    v_next[idx[dest]] += share\n","\n","        if dangling_mass > 0:\n","            v_next += (dangling_mass / n)\n","\n","        v_next = one_minus_m * v_next + teleport\n","\n","        # Normalize at each iteration to avoid numerical drift\n","        s = v_next.sum()\n","        if s != 0:\n","            v_next /= s\n","\n","        last_diff = np.abs(v_next - v).sum()\n","\n","        v = v_next\n","        if last_diff < tol:\n","            break\n","\n","    scores = {node: float(v[idx[node]]) for node in nodes}\n","    return scores, it, last_diff\n","\n","\n","candidates = ['/mnt/data/hollins.dat', 'hollins.dat']\n","file_path = next((p for p in candidates if os.path.exists(p)), None)\n","print(\"file_path =\", file_path)\n"]},{"cell_type":"markdown","id":"0510fddf","metadata":{"id":"0510fddf"},"source":["## Ranking with Ties\n","\n","In some cases, different nodes may obtain identical or nearly identical PageRank values due to symmetry in the link structure or numerical precision limits. A simple sorting of the scores would impose an artificial ordering among such nodes.\n","\n","To address this issue, a tie-aware ranking procedure is introduced. Nodes are first sorted by decreasing PageRank score, and then grouped into the same rank whenever the absolute difference between their scores is below a prescribed tolerance $( \\varepsilon ) $.\n","\n","The ranking scheme adopted here is **competition ranking**: if multiple nodes share the same rank, the next rank is increased by the size of the group. For example, if two nodes are tied at rank 1, the following rank is 3.\n","\n","This approach provides a clearer and more faithful representation of the PageRank results, especially for small example graphs where ties are expected.\n"]},{"cell_type":"code","execution_count":3,"id":"c7af4644","metadata":{"id":"c7af4644","executionInfo":{"status":"ok","timestamp":1767179369244,"user_tz":-60,"elapsed":45,"user":{"displayName":"Erf","userId":"14792764007912314216"}}},"outputs":[],"source":["from typing import Any, Dict, List, Tuple\n","\n","def rank_with_ties(scores: Dict[Any, float], eps: float = 1e-12) -> List[Tuple[int, List[Tuple[Any, float]]]]:\n","    \"\"\"\n","    Groups nodes with (nearly) equal scores into the same rank.\n","    Uses competition ranking: if rank 1 has 2 nodes tied, next rank becomes 3.\n","    \"\"\"\n","    items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n","\n","    groups: List[Tuple[int, List[Tuple[Any, float]]]] = []\n","    rank = 1\n","    i = 0\n","\n","    while i < len(items):\n","        base = items[i][1]\n","        g = [items[i]]\n","        i += 1\n","\n","        while i < len(items) and abs(items[i][1] - base) <= eps:\n","            g.append(items[i])\n","            i += 1\n","\n","        groups.append((rank, g))\n","        rank += len(g)\n","\n","    return groups\n"]},{"cell_type":"markdown","id":"469011ea","metadata":{"id":"469011ea"},"source":["## Application to a Real Dataset\n","\n","The PageRank algorithm is applied to a real web graph provided as an edge-list dataset. After parsing the file, the graph contains 6013 nodes and 23876 directed edges. A significant fraction of the nodes (3189) are dangling nodes, i.e. pages with no outgoing links.\n","\n","The PageRank vector is computed using the power method with damping factor $ (m = 0.15) $ and tolerance $ (10^{-12}) $. Convergence is achieved after 138 iterations, with a final $( \\ell^1)$-difference below the prescribed tolerance. The resulting PageRank vector is properly normalized, with the sum of all scores equal to 1.\n","\n","The top-ranked nodes according to their PageRank scores are reported below. These values indicate the most influential nodes in the graph under the PageRank model. Despite the large number of dangling nodes, the algorithm remains stable and converges reliably due to the uniform redistribution of dangling mass at each iteration.\n","\n","Overall, the results confirm that the implementation scales to larger graphs and preserves the theoretical properties of the PageRank algorithm, including convergence and probabilistic interpretation.\n"]},{"cell_type":"code","execution_count":4,"id":"65bb856e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"65bb856e","executionInfo":{"status":"ok","timestamp":1767179372776,"user_tz":-60,"elapsed":1365,"user":{"displayName":"Erf","userId":"14792764007912314216"}},"outputId":"0071d4b7-d221-48af-8d9a-3bed77aa2487"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded from: hollins.dat\n","Nodes: 6013\n","Edges: 23876\n","Dangling: 3189\n","Iterations: 138\n","Last diff (L1): 9.723798914500204e-13\n","Sum scores: 1.0\n","Top pages by PageRank (dataset):\n"," 1. node 2 (score=1.987463e-02)\n"," 2. node 37 (score=9.285693e-03)\n"," 3. node 38 (score=8.608607e-03)\n"," 4. node 61 (score=8.063358e-03)\n"," 5. node 52 (score=8.024900e-03)\n"," 6. node 43 (score=7.163157e-03)\n"," 7. node 425 (score=6.581415e-03)\n"," 8. node 27 (score=5.987971e-03)\n"," 9. node 28 (score=5.570580e-03)\n","10. node 4023 (score=4.451544e-03)\n"]}],"source":["if file_path:\n","    # Load dataset (edge list). Convention: u -> v means \"u links to v\"\n","    adj_dataset = load_edge_list_as_adj(file_path, flip_direction=False)\n","\n","    num_nodes = len(adj_dataset)\n","    num_edges = sum(len(v) for v in adj_dataset.values())\n","    num_dangling = sum(1 for u, outs in adj_dataset.items() if len(outs) == 0)\n","\n","    print(\"Dataset loaded from:\", file_path)\n","    print(\"Nodes:\", num_nodes)\n","    print(\"Edges:\", num_edges)\n","    print(\"Dangling:\", num_dangling)\n","\n","    scores_dataset, it_dataset, last_diff_dataset = pagerank_power(\n","        adj_dataset, m=0.15, tol=1e-12, max_iter=2000\n","    )\n","\n","    print(\"Iterations:\", it_dataset)\n","    print(\"Last diff (L1):\", last_diff_dataset)\n","    print(\"Sum scores:\", sum(scores_dataset.values()))\n","\n","    # Simple Top 10 (no tie handling)\n","    top10 = sorted(scores_dataset.items(), key=lambda kv: kv[1], reverse=True)[:10]\n","    print(\"Top pages by PageRank (dataset):\")\n","    for i, (node, score) in enumerate(top10, 1):\n","        print(f\"{i:2d}. node {node} (score={score:.6e})\")\n","\n","else:\n","    print(\"Dataset file 'hollins.dat' not found; skipping dataset analysis.\")\n"]},{"cell_type":"markdown","id":"f5a57e32","metadata":{"id":"f5a57e32"},"source":["## Validation on the Example Graphs\n","\n","The PageRank algorithm is first validated on the small example graphs presented in Figures 2.1 and 2.2 of the reference paper. These cases allow a direct comparison with the theoretical results and provide a consistency check for the implementation.\n","\n","### Figure 2.1\n","\n","For the graph in Figure 2.1, the PageRank vector computed with damping factor \\(m = 0.15\\) is\n","$$\n","x \\approx (0.368,\\; 0.142,\\; 0.288,\\; 0.202)\n","$$\n","The algorithm converges in 36 iterations.  \n","The resulting ranking of the pages is:\n","$$\n","1 > 3 > 4 > 2\n","$$\n","\n","This ordering reflects the link structure of the graph: page 1 receives a strong contribution from page 3, which links exclusively to it, increasing its overall importance.\n","\n","---\n","\n","### Figure 2.2\n","\n","For the graph in Figure 2.2, which consists of two disconnected subwebs and an additional page, the computed PageRank vector is\n","$$\n","x = (0.2,\\; 0.2,\\; 0.285,\\; 0.285,\\; 0.03)\n","$$\n","Convergence is achieved after only 2 iterations.\n","\n","Pages 1 and 2 have identical scores, as do pages 3 and 4, reflecting the symmetry of the two subwebs. Page 5 receives the lowest score, since it contributes outgoing links but receives no incoming links.  \n","The introduction of the damping factor ensures a unique PageRank vector despite the disconnected structure of the graph.\n"]},{"cell_type":"code","execution_count":5,"id":"377c4384","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"377c4384","executionInfo":{"status":"ok","timestamp":1767179382129,"user_tz":-60,"elapsed":50,"user":{"displayName":"Erf","userId":"14792764007912314216"}},"outputId":"12bb3d44-532e-4701-8cce-8701bb0e5e5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Figure 2.1 PageRank (m=0.15): [0.36815068 0.14180936 0.28796163 0.20207834]\n","Iterations: 36\n","Ranking: [1 3 4 2]\n","Figure 2.2 PageRank (m=0.15): [0.2   0.2   0.285 0.285 0.03 ]\n","Iterations: 2\n","Ranking: [4 3 2 1 5]\n"]}],"source":["# Compute PageRank on Figure 2.1\n","a1 = build_matrix_fig21()\n","r1, it1 = pagerank(a1, m=0.15)\n","print(\"Figure 2.1 PageRank (m=0.15):\", r1)\n","print(\"Iterations:\", it1)\n","print(\"Ranking:\", np.argsort(r1)[::-1] + 1)\n","\n","# Compute PageRank on Figure 2.2\n","a2 = build_matrix_fig22()\n","r2, it2 = pagerank(a2, m=0.15)\n","print(\"Figure 2.2 PageRank (m=0.15):\", r2)\n","print(\"Iterations:\", it2)\n","print(\"Ranking:\", np.argsort(r2)[::-1] + 1)\n"]},{"cell_type":"markdown","id":"79222c18","metadata":{"id":"79222c18"},"source":["## Exercise 11: PageRank on a Modified Web Graph\n","\n","In Exercise 11, the web graph of Figure 2.1 is modified by adding a fifth page and altering the link structure accordingly. The corresponding link matrix is constructed as described in the exercise, and the PageRank algorithm is applied with damping factor $m = 0.15$.\n","\n","The computed PageRank vector is\n","\n","$$\n","x \\approx (0.237,\\; 0.097,\\; 0.349,\\; 0.138,\\; 0.178)\n","$$\n","\n","and convergence is achieved after 57 iterations.  \n","The resulting ranking of the pages is\n","\n","\\[\n","3 > 1 > 5 > 4 > 2.\n","\\]\n","\n","The introduction of the new page and the modified links significantly changes the importance distribution, particularly increasing the score of page 3, which receives additional influence through the updated structure.\n","\n","---\n","\n","## Exercise 14: Convergence Analysis\n","\n","Exercise 14 focuses on the convergence behavior of the PageRank iteration. Starting from a non-uniform initial vector $x_0$, the sequence\n","\n","$$\n","x^{(k)} = M^{k} x_0\n","$$\n","\n","is compared to the PageRank vector $q$ obtained in Exercise 11.\n","\n","The $\\ell^{1}$-norm of the error $\\| M^{k} x_0 - q \\|_{1}$ is evaluated for selected values of $k$. The results show a rapid decrease of the error, confirming exponential convergence of the power method.\n","\n","In addition, a theoretical contraction bound\n","\n","$$\n","c = \\max_{j} \\left| 1 - 2 \\min_{i} M_{ij} \\right|\n","$$\n","\n","is computed, together with the absolute value of the second largest eigenvalue $|\\lambda_{2}|$ of $M$.\n","\n","The observed convergence rate is consistent with the magnitude of $|\\lambda_{2}|$ and is significantly faster than the pessimistic theoretical bound $c$. This confirms both the correctness of the implementation and the effectiveness of the power method for computing PageRank.\n"]},{"cell_type":"code","execution_count":6,"id":"0f15c298","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"0f15c298","executionInfo":{"status":"ok","timestamp":1767179406162,"user_tz":-60,"elapsed":54,"user":{"displayName":"Erf","userId":"14792764007912314216"}},"outputId":"11a68396-c5ec-4f31-d98e-a88f55a5b560"},"outputs":[{"output_type":"stream","name":"stdout","text":["Exercise 11 PageRank vector: [0.23714058 0.09718983 0.34889409 0.13849551 0.17827999]\n","Iterations: 57\n","Ranking: [3 1 5 4 2]\n","\n","Exercise 14 results:\n","   k |     ||M^k x0 - q||_1 |        ratio\n","------------------------------------------\n","   1 |     4.2184113753e-01 |     0.784400\n","   5 |     4.9672424898e-02 |     0.562539\n","  10 |     4.2036925402e-03 |     0.614189\n","  50 |     1.2052220333e-11 |     0.632505\n","\n","Theoretical contraction bound c = 0.940000\n","|lambda_2| = 0.611269\n"]}],"source":["# Exercise 11: compute PageRank for the modified web\n","a_ex11 = build_matrix_exercise11()\n","a_ex11_stoch = ensure_column_stochastic(a_ex11)\n","q, it_ex11 = pagerank(a_ex11, m=0.15)\n","print(\"Exercise 11 PageRank vector:\", q)\n","print(\"Iterations:\", it_ex11)\n","print(\"Ranking:\", np.argsort(q)[::-1] + 1)\n","\n","# Exercise 14: convergence analysis\n","n = a_ex11.shape[0]\n","m_factor = 0.15\n","S = np.ones((n, n)) / n\n","M_ex11 = (1.0 - m_factor) * a_ex11_stoch + m_factor * S\n","\n","# Initial vector x0\n","t = np.array([0.24, 0.31, 0.08, 0.18, 0.19])\n","x0 = t / t.sum()\n","\n","ks = [1, 5, 10, 50]\n","results = {}\n","vec = x0.copy()\n","prev_err = np.abs(x0 - q).sum()\n","\n","for k in range(1, max(ks) + 1):\n","    vec = M_ex11 @ vec\n","    vec = vec / vec.sum()\n","    err = np.abs(vec - q).sum()\n","    if k in ks:\n","        ratio = err / prev_err if prev_err > 0 else None\n","        results[k] = (err, ratio)\n","    prev_err = err\n","\n","mins_per_col = M_ex11.min(axis=0)\n","c_bound = float(np.max(np.abs(1.0 - 2.0 * mins_per_col)))\n","eigvals = np.linalg.eigvals(M_ex11)\n","abs_eigs = np.sort(np.abs(eigvals))[::-1]\n","lambda2_abs = float(abs_eigs[1])\n","\n","print(\"\\nExercise 14 results:\")\n","print(f\"{'k':>4} | {'||M^k x0 - q||_1':>20} | {'ratio':>12}\")\n","print(\"-\" * 42)\n","for k in ks:\n","    err, ratio = results[k]\n","    ratio_str = '---' if ratio is None else f\"{ratio:.6f}\"\n","    print(f\"{k:4d} | {err:20.10e} | {ratio_str:>12}\")\n","print(f\"\\nTheoretical contraction bound c = {c_bound:.6f}\")\n","print(f\"|lambda_2| = {lambda2_abs:.6f}\")"]},{"cell_type":"markdown","id":"7b3298be","metadata":{"id":"7b3298be"},"source":["## Conclusion\n","\n","In this project, the PageRank algorithm was implemented and analyzed from a linear algebra perspective. Link matrices were constructed for the small web graphs in Figures 2.1 and 2.2, and the corresponding PageRank vectors were computed using the power method. The results are consistent with the theoretical behavior discussed in the reference paper.\n","\n","Exercise 11 demonstrated how modifying the link structure by adding an additional page and new connections can significantly affect the PageRank distribution and the resulting ranking. Exercise 14 provided a numerical study of the convergence of the power method, illustrating its rapid convergence and its dependence on the magnitude of the second largest eigenvalue of the Google matrix.\n","\n","Finally, the implementation was extended to handle real-world graphs represented as edge lists. A dataset loader was included to construct the adjacency structure and handle dangling nodes appropriately. When a dataset file such as `hollins.dat` is available, the PageRank algorithm can be applied efficiently, confirming both the scalability and robustness of the implementation.\n"]},{"cell_type":"code","source":["!cp '/content/drive/MyDrive/Colab Notebooks/pagerank.ipynb' /content/pagerank/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cEPbmZFICI7","executionInfo":{"status":"ok","timestamp":1767179877947,"user_tz":-60,"elapsed":154,"user":{"displayName":"Erf","userId":"14792764007912314216"}},"outputId":"56088056-4312-492a-e61f-135fdedcec92"},"id":"9cEPbmZFICI7","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/drive/MyDrive/Colab Notebooks/pagerank.ipynb': No such file or directory\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_YaxdlyIULa","executionInfo":{"status":"ok","timestamp":1767179965620,"user_tz":-60,"elapsed":65912,"user":{"displayName":"Erf","userId":"14792764007912314216"}},"outputId":"a227b8e1-eaa4-4c96-bba9-354b88b38e7b"},"id":"h_YaxdlyIULa","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.14.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}